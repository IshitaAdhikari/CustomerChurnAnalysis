# -*- coding: utf-8 -*-
"""Customer Churn Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JjKfWr1aDyrqd2YzNQLVIz33D8EkvM6X
"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
df = pd.read_csv(url)

# Display initial information
print("Dataset Shape:", df.shape)
print("\nFirst few rows:")
print(df.head())

"""
 Data Cleaning and Preprocessing"""

# Function to check missing values
def check_missing_values(df):
    print("\nMissing Values:")
    print(df.isnull().sum())

    # Calculate percentage of missing values
    missing_percentage = (df.isnull().sum() / len(df)) * 100
    print("\nMissing Values Percentage:")
    print(missing_percentage[missing_percentage > 0])

# Function to check data types
def check_data_types(df):
    print("\nData Types:")
    print(df.dtypes)

# Function to clean the data
def clean_data(df):
    # Create a copy of the dataframe
    df_clean = df.copy()

    # Convert TotalCharges to numeric
    df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')

    # Fill missing values in TotalCharges with MonthlyCharges
    df_clean['TotalCharges'].fillna(df_clean['MonthlyCharges'], inplace=True)

    # Convert binary variables
    binary_cols = ['gender', 'PhoneService', 'PaperlessBilling', 'Churn']
    for col in binary_cols:
        df_clean[col] = df_clean[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})

    # Convert categorical variables using one-hot encoding
    cat_cols = ['InternetService', 'Contract', 'PaymentMethod']
    df_clean = pd.get_dummies(df_clean, columns=cat_cols)

    return df_clean

# Function to scale numeric features
def scale_features(df):
    scaler = StandardScaler()
    numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    return df

# Apply the functions
print("Step 1: Checking Missing Values")
check_missing_values(df)

print("\nStep 2: Checking Data Types")
check_data_types(df)

print("\nStep 3: Cleaning Data")
df_cleaned = clean_data(df)

print("\nStep 4: Scaling Features")
df_processed = scale_features(df_cleaned)

print("\nProcessed Data Shape:", df_processed.shape)
print("\nProcessed Columns:", df_processed.columns.tolist())

# Save processed data for next steps
df_processed.to_csv('processed_telco_data.csv', index=False)
print("\nProcessed data saved successfully!")

# Display sample of processed data
print("\nSample of processed data:")
print(df_processed.head())

"""Exploratory Data Analysis (EDA)"""

# Load the processed data
df = pd.read_csv('processed_telco_data.csv')

# 1. Basic Statistical Analysis
def basic_analysis(df):
    print("\n=== Basic Statistical Analysis ===")
    print("\nNumerical Features Summary:")
    print(df[['tenure', 'MonthlyCharges', 'TotalCharges']].describe())

    print("\nChurn Distribution:")
    print(df['Churn'].value_counts(normalize=True) * 100)

# 2. Visualization Functions
def plot_churn_distribution(df):
    plt.figure(figsize=(10, 6))
    sns.countplot(data=df, x='Churn')
    plt.title('Customer Churn Distribution')
    plt.xlabel('Churn (0: No, 1: Yes)')
    plt.ylabel('Count')
    plt.show()

def plot_numeric_features(df):
    numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

    plt.figure(figsize=(15, 5))
    for i, col in enumerate(numeric_cols, 1):
        plt.subplot(1, 3, i)
        sns.boxplot(data=df, x='Churn', y=col)
        plt.title(f'{col} vs Churn')
    plt.tight_layout()
    plt.show()

def plot_correlation_matrix(df):
    numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']
    correlation = df[numeric_cols].corr()

    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation Matrix')
    plt.show()

def plot_categorical_analysis(df):
    categorical_cols = [col for col in df.columns if col.startswith(('InternetService_', 'Contract_', 'PaymentMethod_'))]

    plt.figure(figsize=(15, 10))
    for i, col in enumerate(categorical_cols, 1):
        plt.subplot(4, 3, i)
        sns.barplot(data=df, x=col, y='Churn')
        plt.title(f'{col} vs Churn')
        plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# 3. Customer Segments Analysis
def analyze_customer_segments(df):
    print("\n=== Customer Segments Analysis ===")

    # Tenure segments
    df['tenure_segment'] = pd.qcut(df['tenure'], q=4, labels=['0-25%', '25-50%', '50-75%', '75-100%'])

    print("\nChurn Rate by Tenure Segment:")
    print(df.groupby('tenure_segment')['Churn'].mean() * 100)

    # Monthly charges segments
    df['charges_segment'] = pd.qcut(df['MonthlyCharges'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])

    print("\nChurn Rate by Monthly Charges Segment:")
    print(df.groupby('charges_segment')['Churn'].mean() * 100)

# Execute all analyses
print("Starting Exploratory Data Analysis...")

# Run basic analysis
basic_analysis(df)

# Create visualizations
print("\nGenerating visualizations...")
plot_churn_distribution(df)
plot_numeric_features(df)
plot_correlation_matrix(df)
plot_categorical_analysis(df)

# Analyze customer segments
analyze_customer_segments(df)

# Additional insights
print("\n=== Key Insights ===")
print(f"Overall Churn Rate: {df['Churn'].mean()*100:.2f}%")
print(f"Average Monthly Charges: ${df['MonthlyCharges'].mean():.2f}")
print(f"Average Tenure: {df['tenure'].mean():.1f} months")

# Save key metrics for later use
metrics = {
    'churn_rate': df['Churn'].mean(),
    'avg_monthly_charges': df['MonthlyCharges'].mean(),
    'avg_tenure': df['tenure'].mean()
}

print("\nEDA completed successfully!")

"""Feature Engineering"""

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# Load the data
url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
df = pd.read_csv(url)

def preprocess_data(df):
    """Preprocess the data"""
    print("Preprocessing data...")

    # Create a copy
    df_clean = df.copy()

    # Convert TotalCharges to numeric
    df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')
    df_clean['TotalCharges'].fillna(0, inplace=True)

    # Convert binary variables
    le = LabelEncoder()
    binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService',
                  'PaperlessBilling', 'Churn']

    for col in binary_cols:
        df_clean[col] = le.fit_transform(df_clean[col])

    # Convert categorical variables to dummy variables
    cat_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity',
                'OnlineBackup', 'DeviceProtection', 'TechSupport',
                'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']

    df_clean = pd.get_dummies(df_clean, columns=cat_cols, prefix=cat_cols)

    return df_clean

def create_features(df):
    """Create new features"""
    print("Creating new features...")

    df_featured = df.copy()

    # Avoid division by zero
    df_featured['tenure'] = df_featured['tenure'].replace(0, 1)

    # Create new features
    df_featured['AvgMonthlyCharges'] = df_featured['TotalCharges'] / df_featured['tenure']
    df_featured['TenureYears'] = df_featured['tenure'] / 12
    df_featured['TotalServices'] = df_featured.filter(like='_Yes').sum(axis=1)

    return df_featured

def scale_features(df):
    """Scale numeric features"""
    print("Scaling features...")

    # Create a copy
    df_scaled = df.copy()

    # Select numeric columns
    numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges',
                   'AvgMonthlyCharges', 'TenureYears']

    # Scale numeric features
    scaler = StandardScaler()
    df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])

    return df_scaled

def select_features(df):
    """Select final features"""
    print("Selecting features...")

    # Drop customer ID
    df_selected = df.drop('customerID', axis=1)

    return df_selected

# Execute feature engineering pipeline
print("Starting feature engineering process...")

# 1. Preprocess data
df_processed = preprocess_data(df)
print("Preprocessing completed.")

# 2. Create new features
df_featured = create_features(df_processed)
print("Feature creation completed.")

# 3. Scale features
df_scaled = scale_features(df_featured)
print("Feature scaling completed.")

# 4. Select final features
df_final = select_features(df_scaled)
print("Feature selection completed.")

# Print summary
print("\nFeature Engineering Summary:")
print(f"Original number of features: {df.shape[1]}")
print(f"Final number of features: {df_final.shape[1]}")

# Analyze feature importance
def analyze_feature_importance(df_final):
    """Analyze feature importance"""
    print("\nAnalyzing feature importance...")

    X = df_final.drop('Churn', axis=1)
    y = df_final['Churn']

    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)

    importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)

    return importance

# Save the engineered features
df_final.to_csv('engineered_features.csv', index=False)
print("\nEngineered features saved successfully!")

# Get feature importance
feature_importance = analyze_feature_importance(df_final)

# Display results
print("\nTop 10 Most Important Features:")
print(feature_importance.head(10))

print("\nSample of final dataset:")
print(df_final.head())

"""Model Building and Evaluation"""

# Import additional libraries
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load the engineered features
df = pd.read_csv('engineered_features.csv')

def split_data(df):
    """Split data into training and testing sets"""
    print("Splitting data into train and test sets...")

    X = df.drop('Churn', axis=1)
    y = df['Churn']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    return X_train, X_test, y_train, y_test

def train_models(X_train, y_train):
    """Train multiple models"""
    print("Training multiple models...")

    models = {
        'Random Forest': RandomForestClassifier(random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'XGBoost': XGBClassifier(random_state=42)
    }

    trained_models = {}
    for name, model in models.items():
        print(f"\nTraining {name}...")
        model.fit(X_train, y_train)
        trained_models[name] = model

    return trained_models

def evaluate_models(models, X_test, y_test):
    """Evaluate models performance"""
    print("\nModel Evaluation:")

    results = {}
    for name, model in models.items():
        print(f"\n{name} Results:")

        # Make predictions
        y_pred = model.predict(X_test)

        # Classification report
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))

        # Confusion Matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(confusion_matrix(y_test, y_pred),
                   annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix - {name}')
        plt.show()

        # ROC Curve
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {name}')
        plt.legend()
        plt.show()

        results[name] = {
            'auc': roc_auc,
            'model': model
        }

    return results

def tune_best_model(X_train, y_train, best_model_name):
    """Tune the best performing model"""
    print(f"\nTuning {best_model_name}...")

    if best_model_name == 'Random Forest':
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        model = RandomForestClassifier(random_state=42)

    elif best_model_name == 'XGBoost':
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 4, 5],
            'learning_rate': [0.01, 0.1, 0.3]
        }
        model = XGBClassifier(random_state=42)

    else:  # Logistic Regression
        param_grid = {
            'C': [0.001, 0.01, 0.1, 1, 10],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga']
        }
        model = LogisticRegression(random_state=42)

    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    print("\nBest parameters:", grid_search.best_params_)
    print("Best cross-validation score:", grid_search.best_score_)

    return grid_search.best_estimator_

# Execute model building pipeline
print("Starting model building process...")

# 1. Split the data
X_train, X_test, y_train, y_test = split_data(df)

# 2. Train multiple models
trained_models = train_models(X_train, y_train)

# 3. Evaluate models
results = evaluate_models(trained_models, X_test, y_test)

# 4. Find best model
best_model_name = max(results.items(), key=lambda x: x[1]['auc'])[0]
print(f"\nBest performing model: {best_model_name}")

# 5. Tune best model
best_model = tune_best_model(X_train, y_train, best_model_name)

# 6. Final evaluation of tuned model
print("\nFinal Model Evaluation:")
y_pred = best_model.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Save the best model
import joblib
joblib.dump(best_model, 'best_model.joblib')
print("\nBest model saved successfully!")

# Feature importance of the final model
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')
    plt.title('Top 10 Most Important Features')
    plt.show()

"""Model Deployment and Prediction System"""

# Save your best model
import joblib
joblib.dump(best_model, 'best_model.joblib')

# Import required libraries
import gradio as gr
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

# Load the original dataset
url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
df = pd.read_csv(url)

# Data preprocessing
def preprocess_data(df):
    # Create a copy
    df_clean = df.copy()

    # Convert TotalCharges to numeric
    df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')
    df_clean['TotalCharges'].fillna(0, inplace=True)

    # Convert binary variables
    le = LabelEncoder()
    binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService',
                   'PaperlessBilling', 'Churn']

    for col in binary_cols:
        df_clean[col] = le.fit_transform(df_clean[col])

    # Convert categorical variables to dummy variables
    cat_cols = ['InternetService', 'OnlineSecurity', 'OnlineBackup',
                'DeviceProtection', 'TechSupport', 'StreamingTV',
                'StreamingMovies', 'Contract', 'PaymentMethod', 'MultipleLines']

    df_clean = pd.get_dummies(df_clean, columns=cat_cols)

    return df_clean

# Preprocess the data
df_processed = preprocess_data(df)

# Prepare features and target
X = df_processed.drop(['customerID', 'Churn'], axis=1)
y = df_processed['Churn']

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# Create prediction function
def predict_churn(gender, senior_citizen, partner, dependents, tenure,
                 phone_service, internet_service, contract, paperless_billing,
                 payment_method, monthly_charges):
    """Make prediction using the trained model"""

    # Create a dictionary with the input data
    input_data = {
        'gender': 1 if gender == 'Male' else 0,
        'SeniorCitizen': senior_citizen,
        'Partner': 1 if partner == 'Yes' else 0,
        'Dependents': 1 if dependents == 'Yes' else 0,
        'tenure': tenure,
        'PhoneService': 1 if phone_service == 'Yes' else 0,
        'PaperlessBilling': 1 if paperless_billing == 'Yes' else 0,
        'MonthlyCharges': monthly_charges,

        # Initialize all service columns to 0
        'InternetService_DSL': 0,
        'InternetService_Fiber optic': 0,
        'InternetService_No': 0,
        'Contract_Month-to-month': 0,
        'Contract_One year': 0,
        'Contract_Two year': 0,
        'PaymentMethod_Bank transfer (automatic)': 0,
        'PaymentMethod_Credit card (automatic)': 0,
        'PaymentMethod_Electronic check': 0,
        'PaymentMethod_Mailed check': 0
    }

    # Set the appropriate internet service
    if internet_service != 'No':
        input_data[f'InternetService_{internet_service}'] = 1
    else:
        input_data['InternetService_No'] = 1

    # Set the contract type
    input_data[f'Contract_{contract}'] = 1

    # Set the payment method
    input_data[f'PaymentMethod_{payment_method}'] = 1

    # Convert to DataFrame
    input_df = pd.DataFrame([input_data])

    # Ensure columns match training data
    input_df = input_df.reindex(columns=X.columns, fill_value=0)

    # Make prediction
    prediction = model.predict(input_df)[0]
    probability = model.predict_proba(input_df)[0][1]

    # Calculate risk factors
    risk_factors = []
    if contract == 'Month-to-month':
        risk_factors.append('Month-to-month contract')
    if tenure < 12:
        risk_factors.append('Low tenure (less than 1 year)')
    if monthly_charges > 100:
        risk_factors.append('High monthly charges')

    # Format result message
    result = f"""
    ## Prediction Results

    {'⚠️ High Risk of Churn' if prediction == 1 else '✅ Low Risk of Churn'}
    Churn Probability: {probability:.2%}

    ## Customer Profile
    - Gender: {gender}
    - Tenure: {tenure} months
    - Contract Type: {contract}
    - Internet Service: {internet_service}

    ## Financial Information
    - Monthly Charges: ${monthly_charges:.2f}
    - Payment Method: {payment_method}

    ## Risk Factors
    """

    if risk_factors:
        result += "\nKey risk factors identified:\n"
        for factor in risk_factors:
            result += f"- {factor}\n"
    else:
        result += "\nNo significant risk factors identified."

    return result

# Create Gradio interface
demo = gr.Interface(
    fn=predict_churn,
    inputs=[
        gr.Radio(["Male", "Female"], label="Gender"),
        gr.Radio([0, 1], label="Senior Citizen"),
        gr.Radio(["Yes", "No"], label="Partner"),
        gr.Radio(["Yes", "No"], label="Dependents"),
        gr.Slider(0, 72, step=1, label="Tenure (months)"),
        gr.Radio(["Yes", "No"], label="Phone Service"),
        gr.Radio(["DSL", "Fiber optic", "No"], label="Internet Service"),
        gr.Radio(["Month-to-month", "One year", "Two year"], label="Contract"),
        gr.Radio(["Yes", "No"], label="Paperless Billing"),
        gr.Radio([
            "Electronic check",
            "Mailed check",
            "Bank transfer (automatic)",
            "Credit card (automatic)"
        ], label="Payment Method"),
        gr.Slider(0, 200, step=1, label="Monthly Charges ($)")
    ],
    outputs=gr.Markdown(),
    title="Telco Customer Churn Predictor",
    description="Enter customer information to predict churn probability",
    examples=[
        # Example 1: High risk customer
        ["Male", 0, "No", "No", 1, "Yes", "Fiber optic", "Month-to-month",
         "Yes", "Electronic check", 150],
        # Example 2: Low risk customer
        ["Female", 0, "Yes", "Yes", 48, "Yes", "DSL", "Two year",
         "No", "Bank transfer (automatic)", 65]
    ]
)

# Launch the app
demo.launch(share=True)